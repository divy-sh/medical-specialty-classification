{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kagglehub in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (0.3.10)\n",
      "Requirement already satisfied: pyyaml in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: tqdm in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: packaging in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: requests in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests->kagglehub) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests->kagglehub) (3.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: click in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from sklearn) (1.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (2.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sklearn) (1.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (2.19.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.13.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.9.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.0.0)\n",
      "Requirement already satisfied: packaging in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.2.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: rich in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: optree in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: namex in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting yake\n",
      "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 2.0 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-1.2.0-cp39-cp39-macosx_11_0_arm64.whl (325 kB)\n",
      "\u001b[K     |████████████████████████████████| 325 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click>=6.0 in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from yake) (8.1.8)\n",
      "Requirement already satisfied: numpy in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from yake) (2.0.2)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting segtok\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: regex in /Users/divyendu/Library/Python/3.9/lib/python/site-packages (from segtok->yake) (2024.11.6)\n",
      "Installing collected packages: tabulate, segtok, networkx, jellyfish, yake\n",
      "Successfully installed jellyfish-1.2.0 networkx-3.2.1 segtok-1.5.11 tabulate-0.9.0 yake-0.4.8\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install required dependencies\n",
    "%pip install kagglehub\n",
    "%pip install pandas\n",
    "%pip install nltk\n",
    "%pip install sklearn\n",
    "%pip install tensorflow\n",
    "%pip install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/divyendu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all the required dependencies\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/divyendu/.cache/kagglehub/datasets/tboyle10/medicaltranscriptions/versions/1\n",
      "Head:  <bound method NDFrame.head of       Unnamed: 0                                        description  \\\n",
      "0              0   A 23-year-old white female presents with comp...   \n",
      "1              1           Consult for laparoscopic gastric bypass.   \n",
      "2              2           Consult for laparoscopic gastric bypass.   \n",
      "3              3                             2-D M-Mode. Doppler.     \n",
      "4              4                                 2-D Echocardiogram   \n",
      "...          ...                                                ...   \n",
      "4994        4994   Patient having severe sinusitis about two to ...   \n",
      "4995        4995   This is a 14-month-old baby boy Caucasian who...   \n",
      "4996        4996   A female for a complete physical and follow u...   \n",
      "4997        4997   Mother states he has been wheezing and coughing.   \n",
      "4998        4998   Acute allergic reaction, etiology uncertain, ...   \n",
      "\n",
      "                medical_specialty                                sample_name  \\\n",
      "0            Allergy / Immunology                         Allergic Rhinitis    \n",
      "1                      Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
      "2                      Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
      "3      Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
      "4      Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
      "...                           ...                                        ...   \n",
      "4994         Allergy / Immunology                         Chronic Sinusitis    \n",
      "4995         Allergy / Immunology      Kawasaki Disease - Discharge Summary    \n",
      "4996         Allergy / Immunology                        Followup on Asthma    \n",
      "4997         Allergy / Immunology                    Asthma in a 5-year-old    \n",
      "4998         Allergy / Immunology                Allergy Evaluation Consult    \n",
      "\n",
      "                                          transcription  \\\n",
      "0     SUBJECTIVE:,  This 23-year-old white female pr...   \n",
      "1     PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
      "2     HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n",
      "3     2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
      "4     1.  The left ventricular cavity size and wall ...   \n",
      "...                                                 ...   \n",
      "4994  HISTORY:,  I had the pleasure of meeting and e...   \n",
      "4995  ADMITTING DIAGNOSIS: , Kawasaki disease.,DISCH...   \n",
      "4996  SUBJECTIVE: , This is a 42-year-old white fema...   \n",
      "4997  CHIEF COMPLAINT: , This 5-year-old male presen...   \n",
      "4998  HISTORY: , A 34-year-old male presents today s...   \n",
      "\n",
      "                                               keywords  \n",
      "0     allergy / immunology, allergic rhinitis, aller...  \n",
      "1     bariatrics, laparoscopic gastric bypass, weigh...  \n",
      "2     bariatrics, laparoscopic gastric bypass, heart...  \n",
      "3     cardiovascular / pulmonary, 2-d m-mode, dopple...  \n",
      "4     cardiovascular / pulmonary, 2-d, doppler, echo...  \n",
      "...                                                 ...  \n",
      "4994                                                NaN  \n",
      "4995  allergy / immunology, mucous membranes, conjun...  \n",
      "4996                                                NaN  \n",
      "4997                                                NaN  \n",
      "4998                                                NaN  \n",
      "\n",
      "[4999 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"tboyle10/medicaltranscriptions\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "dataset = pd.read_csv(path + \"/mtsamples.csv\")\n",
    "print(\"Head: \", dataset.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data llmlmlmlmlmlm\n",
    "\n",
    "dataset.dropna(subset=['transcription', 'medical_specialty'], inplace=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "dataset['transcription'] = dataset['transcription'].apply(clean_text)\n",
    "dataset['medical_specialty'] = dataset['medical_specialty'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0: cardiovascular pulmonary: 371\n",
      "Category 1: consult history phy: 516\n",
      "Category 2: discharge summary: 108\n",
      "Category 3: emergency room reports: 75\n",
      "Category 4: ent otolaryngology: 96\n",
      "Category 5: gastroenterology: 224\n",
      "Category 6: general medicine: 259\n",
      "Category 7: hematology oncology: 90\n",
      "Category 8: nephrology: 81\n",
      "Category 9: neurology: 223\n",
      "Category 10: neurosurgery: 94\n",
      "Category 11: obstetrics gynecology: 155\n",
      "Category 12: office notes: 50\n",
      "Category 13: ophthalmology: 83\n",
      "Category 14: orthopedic: 355\n",
      "Category 15: pain management: 61\n",
      "Category 16: pediatrics neonatal: 70\n",
      "Category 17: psychiatry psychology: 53\n",
      "Category 18: radiology: 273\n",
      "Category 19: soap chart progress notes: 166\n",
      "Category 20: surgery: 1088\n",
      "Category 21: urology: 156\n"
     ]
    }
   ],
   "source": [
    "specialty_counts = dataset['medical_specialty'].value_counts()\n",
    "valid_specialties = specialty_counts[specialty_counts >= 50].index\n",
    "dataset = dataset[dataset['medical_specialty'].isin(valid_specialties)]\n",
    "categories = dataset.groupby(dataset[\"medical_specialty\"])\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    category_name, category = cat\n",
    "    print(f\"Category {i}: {category_name}: {len(category)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3774\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      " cardiovascular pulmonary       0.52      0.41      0.45        74\n",
      "      consult history phy       0.29      0.89      0.44       103\n",
      "        discharge summary       0.30      0.14      0.19        22\n",
      "   emergency room reports       0.00      0.00      0.00        15\n",
      "       ent otolaryngology       0.00      0.00      0.00        19\n",
      "         gastroenterology       0.43      0.29      0.35        45\n",
      "         general medicine       0.17      0.02      0.03        52\n",
      "      hematology oncology       0.00      0.00      0.00        18\n",
      "               nephrology       0.00      0.00      0.00        16\n",
      "                neurology       0.41      0.36      0.38        45\n",
      "             neurosurgery       0.00      0.00      0.00        19\n",
      "    obstetrics gynecology       0.44      0.13      0.20        31\n",
      "             office notes       0.00      0.00      0.00        10\n",
      "            ophthalmology       0.00      0.00      0.00        17\n",
      "               orthopedic       0.30      0.25      0.27        71\n",
      "          pain management       0.00      0.00      0.00        12\n",
      "      pediatrics neonatal       0.00      0.00      0.00        14\n",
      "    psychiatry psychology       0.00      0.00      0.00        10\n",
      "                radiology       0.38      0.33      0.35        55\n",
      "soap chart progress notes       0.00      0.00      0.00        33\n",
      "                  surgery       0.45      0.71      0.55       218\n",
      "                  urology       1.00      0.03      0.06        31\n",
      "\n",
      "                 accuracy                           0.38       930\n",
      "                macro avg       0.21      0.16      0.15       930\n",
      "             weighted avg       0.33      0.38      0.31       930\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyendu/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/divyendu/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/divyendu/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset['transcription'], dataset['medical_specialty'], test_size=0.2, random_state=42, stratify=dataset['medical_specialty']\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidaataset', TfidfVectorizer(max_features=1000)),  # Convert text to TF-Idaataset features\n",
    "    ('clf', MultinomialNB())  # Train a Naive Bayes classifier\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyendu/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 157ms/step - accuracy: 0.1824 - loss: 3.4791 - val_accuracy: 0.3534 - val_loss: 3.0453\n",
      "Epoch 2/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 158ms/step - accuracy: 0.3106 - loss: 2.4766 - val_accuracy: 0.3726 - val_loss: 3.0287\n",
      "Epoch 3/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.3258 - loss: 2.2976 - val_accuracy: 0.3452 - val_loss: 3.0011\n",
      "Epoch 4/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.3815 - loss: 2.1720 - val_accuracy: 0.3726 - val_loss: 2.9638\n",
      "Epoch 5/5\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.3625 - loss: 2.0901 - val_accuracy: 0.3644 - val_loss: 2.8991\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3643 - loss: 2.9044\n",
      "Accuracy: 0.3644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, BatchNormalization, Dropout\n",
    "import yake\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "dataset['encoded_label'] = label_encoder.fit_transform(dataset['medical_specialty'])\n",
    "dataset.dropna(subset=['keywords'], inplace=True)\n",
    "\n",
    "def extract_keywords(text):\n",
    "    kw_extractor = yake.KeywordExtractor(n=1, top=50)  # Extract top 10 single-word keywords\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return ' '.join([kw[0] for kw in keywords])  # Return keywords as a space-separated string\n",
    "# Apply keyword extraction\n",
    "dataset['new_keywords'] = dataset['transcription'].apply(extract_keywords)\n",
    "\n",
    "dataset['new_keywords'] = dataset['new_keywords'].apply(lambda x: ' '.join(x))\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset['new_keywords'], dataset['encoded_label'], test_size=0.1, random_state=42, stratify=dataset['encoded_label']\n",
    ")\n",
    "\n",
    "max_words = 20000  # Increased vocabulary size\n",
    "max_len = 300  # Increased sequence length\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)\n",
    "# Build improved CNN model\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=256, input_length=max_len),\n",
    "    Conv1D(256, 5, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train model\n",
    "model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_data=(X_test_seq, y_test))\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test_seq, y_test)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cleaned_transcription'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cleaned_transcription'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Combine transcription and sample name for keyword extraction\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_transcription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_sample_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# YAKE Keyword Extraction\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_keywords\u001b[39m(text):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cleaned_transcription'"
     ]
    }
   ],
   "source": [
    "# Combine transcription and sample name for keyword extraction\n",
    "dataset['combined_text'] = dataset['cleaned_transcription'] + \" \" + dataset['cleaned_sample_name']\n",
    "\n",
    "# YAKE Keyword Extraction\n",
    "def extract_keywords(text):\n",
    "    kw_extractor = yake.KeywordExtractor(n=2, top=10)  # Extract top 10 bi-gram keywords\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return ' '.join([kw[0] for kw in keywords])  # Return keywords as a space-separated string\n",
    "\n",
    "# Apply YAKE for keyword extraction\n",
    "dataset['keywords'] = dataset['combined_text'].apply(extract_keywords)\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "dataset['encoded_label'] = label_encoder.fit_transform(dataset['medical_specialty'])\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset['keywords'], dataset['encoded_label'], test_size=0.2, random_state=42, stratify=dataset['encoded_label']\n",
    ")\n",
    "\n",
    "# Tokenization\n",
    "max_words = 20000  # Increased vocabulary size\n",
    "max_len = 50  # Reduced sequence length since keywords are shorter\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)\n",
    "\n",
    "# Build improved CNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=256, input_length=max_len),\n",
    "    Conv1D(128, 5, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train model\n",
    "model.fit(X_train_seq, y_train, epochs=10, batch_size=128, validation_data=(X_test_seq, y_test))\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test_seq, y_test)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m X_test_seq \u001b[38;5;241m=\u001b[39m pad_sequences(tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_test), maxlen\u001b[38;5;241m=\u001b[39mmax_len)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Build improved CNN model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m     11\u001b[0m     Embedding(input_dim\u001b[38;5;241m=\u001b[39mmax_words, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, input_length\u001b[38;5;241m=\u001b[39mmax_len),\n\u001b[0;32m---> 12\u001b[0m     Conv1D(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39m\u001b[43ml2\u001b[49m(\u001b[38;5;241m0.001\u001b[39m)),\n\u001b[1;32m     13\u001b[0m     BatchNormalization(),\n\u001b[1;32m     14\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     15\u001b[0m     GlobalMaxPooling1D(),\n\u001b[1;32m     16\u001b[0m     Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39ml2(\u001b[38;5;241m0.001\u001b[39m)),\n\u001b[1;32m     17\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     18\u001b[0m     Dense(\u001b[38;5;28mlen\u001b[39m(label_encoder\u001b[38;5;241m.\u001b[39mclasses_), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m ])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compile model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l2' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenization\n",
    "max_words = 20000  # Increased vocabulary size\n",
    "max_len = 500  # Reduced sequence length since keywords are shorter\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)\n",
    "\n",
    "# Build improved CNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=256, input_length=max_len),\n",
    "    Conv1D(128, 5, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train model\n",
    "model.fit(X_train_seq, y_train, epochs=20, batch_size=128, validation_data=(X_test_seq, y_test))\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test_seq, y_test)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
