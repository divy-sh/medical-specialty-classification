{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33292,
     "status": "ok",
     "timestamp": 1743545451356,
     "user": {
      "displayName": "divyendu shekhar",
      "userId": "03175873153716445422"
     },
     "user_tz": 240
    },
    "id": "31kgUE4KFLfe",
    "outputId": "3a94dd04-ea93-4027-fd3d-5338582a32c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.10)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: tf-keras in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# install required dependencies\n",
    "%pip install kagglehub\n",
    "%pip install pandas\n",
    "%pip install nltk\n",
    "%pip install sklearn\n",
    "%pip install tensorflow\n",
    "%pip install matplotlib\n",
    "%pip install tf-keras\n",
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14151,
     "status": "ok",
     "timestamp": 1743547470373,
     "user": {
      "displayName": "divyendu shekhar",
      "userId": "03175873153716445422"
     },
     "user_tz": 240
    },
    "id": "gQLHDLkJFLfg",
    "outputId": "3a5799ea-54c3-4920-ba94-0a8573429c70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all the required dependencies\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1945,
     "status": "ok",
     "timestamp": 1743552121575,
     "user": {
      "displayName": "divyendu shekhar",
      "userId": "03175873153716445422"
     },
     "user_tz": 240
    },
    "id": "fMwqXukyFLfg",
    "outputId": "98bba501-0a8b-4657-db82-63b211bc2797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/medicaltranscriptions\n",
      "Head:  <bound method NDFrame.head of       Unnamed: 0                                        description  \\\n",
      "0              0   A 23-year-old white female presents with comp...   \n",
      "1              1           Consult for laparoscopic gastric bypass.   \n",
      "2              2           Consult for laparoscopic gastric bypass.   \n",
      "3              3                             2-D M-Mode. Doppler.     \n",
      "4              4                                 2-D Echocardiogram   \n",
      "...          ...                                                ...   \n",
      "4994        4994   Patient having severe sinusitis about two to ...   \n",
      "4995        4995   This is a 14-month-old baby boy Caucasian who...   \n",
      "4996        4996   A female for a complete physical and follow u...   \n",
      "4997        4997   Mother states he has been wheezing and coughing.   \n",
      "4998        4998   Acute allergic reaction, etiology uncertain, ...   \n",
      "\n",
      "                medical_specialty                                sample_name  \\\n",
      "0            Allergy / Immunology                         Allergic Rhinitis    \n",
      "1                      Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
      "2                      Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
      "3      Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
      "4      Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
      "...                           ...                                        ...   \n",
      "4994         Allergy / Immunology                         Chronic Sinusitis    \n",
      "4995         Allergy / Immunology      Kawasaki Disease - Discharge Summary    \n",
      "4996         Allergy / Immunology                        Followup on Asthma    \n",
      "4997         Allergy / Immunology                    Asthma in a 5-year-old    \n",
      "4998         Allergy / Immunology                Allergy Evaluation Consult    \n",
      "\n",
      "                                          transcription  \\\n",
      "0     SUBJECTIVE:,  This 23-year-old white female pr...   \n",
      "1     PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
      "2     HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n",
      "3     2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
      "4     1.  The left ventricular cavity size and wall ...   \n",
      "...                                                 ...   \n",
      "4994  HISTORY:,  I had the pleasure of meeting and e...   \n",
      "4995  ADMITTING DIAGNOSIS: , Kawasaki disease.,DISCH...   \n",
      "4996  SUBJECTIVE: , This is a 42-year-old white fema...   \n",
      "4997  CHIEF COMPLAINT: , This 5-year-old male presen...   \n",
      "4998  HISTORY: , A 34-year-old male presents today s...   \n",
      "\n",
      "                                               keywords  \n",
      "0     allergy / immunology, allergic rhinitis, aller...  \n",
      "1     bariatrics, laparoscopic gastric bypass, weigh...  \n",
      "2     bariatrics, laparoscopic gastric bypass, heart...  \n",
      "3     cardiovascular / pulmonary, 2-d m-mode, dopple...  \n",
      "4     cardiovascular / pulmonary, 2-d, doppler, echo...  \n",
      "...                                                 ...  \n",
      "4994                                                NaN  \n",
      "4995  allergy / immunology, mucous membranes, conjun...  \n",
      "4996                                                NaN  \n",
      "4997                                                NaN  \n",
      "4998                                                NaN  \n",
      "\n",
      "[4999 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"tboyle10/medicaltranscriptions\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "dataset = pd.read_csv(path + \"/mtsamples.csv\")\n",
    "print(\"Head: \", dataset.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgaYplIzFLfh"
   },
   "source": [
    "## Data processing\n",
    "\n",
    "- We drop every other column except transcription and medical_specialty.\n",
    "- We also drop any rows with empty or null transcription or medical_specialty.\n",
    "\n",
    "- Then we drop all the classes in the excluded specialties list below. We do this as these are general terms and don't specifically map to any specialty.\n",
    "- We then merge the classes with large overlaps - e.g. Neurosurgery and neurology, Neurosurgery is a subset of neurology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1743553198045,
     "user": {
      "displayName": "divyendu shekhar",
      "userId": "03175873153716445422"
     },
     "user_tz": 240
    },
    "id": "btKFXrJNFLfm",
    "outputId": "51e56928-44ac-4b1e-e68a-598ccffcbe76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0: Cardiovascular / Pulmonary: 371\n",
      "Category 1: ENT - Otolaryngology: 96\n",
      "Category 2: Gastroenterology: 224\n",
      "Category 3: Hematology - Oncology: 90\n",
      "Category 4: Neurology: 317\n",
      "Category 5: Obstetrics / Gynecology: 155\n",
      "Category 6: Ophthalmology: 83\n",
      "Category 7: Orthopedic: 355\n",
      "Category 8: Pediatrics - Neonatal: 70\n",
      "Category 9: Podiatry: 47\n",
      "Category 10: Psychiatry / Psychology: 53\n",
      "Category 11: Urology: 237\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values in specified columns\n",
    "dataset.dropna(subset=['transcription', 'medical_specialty'], inplace=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "dataset = dataset[['transcription', 'medical_specialty']]\n",
    "\n",
    "# Filter medical specialties with at least 30 occurrences\n",
    "specialty_counts = dataset['medical_specialty'].value_counts()\n",
    "valid_specialties = specialty_counts[specialty_counts >= 30].index\n",
    "dataset = dataset[dataset['medical_specialty'].isin(valid_specialties)]\n",
    "\n",
    "# Strip spaces in 'medical_specialty' column\n",
    "dataset['medical_specialty'] = dataset['medical_specialty'].str.strip()\n",
    "\n",
    "# Remove specific categories\n",
    "excluded_specialties = [\n",
    "    'Surgery',\n",
    "    'SOAP / Chart / Progress Notes',\n",
    "    'Office Notes',\n",
    "    'Consult - History and Phy.',\n",
    "    'Emergency Room Reports',\n",
    "    'Discharge Summary',\n",
    "    'Pain Management',\n",
    "    'General Medicine',\n",
    "    'Radiology',\n",
    "]\n",
    "\n",
    "dataset = dataset[~dataset['medical_specialty'].isin(excluded_specialties)]\n",
    "\n",
    "# Define category mapping to merge similar categories\n",
    "category_mapping = {\n",
    "    'Neurosurgery': 'Neurology',\n",
    "    'Nephrology': 'Urology',\n",
    "}\n",
    "\n",
    "# Apply category mapping\n",
    "dataset['medical_specialty'] = dataset['medical_specialty'].replace(category_mapping)\n",
    "\n",
    "# Display counts for each category\n",
    "for i, (category_name, category) in enumerate(dataset.groupby(\"medical_specialty\")):\n",
    "    print(f\"Category {i}: {category_name}: {len(category)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvAv-4CwFLfm"
   },
   "source": [
    "## Data processing\n",
    "\n",
    "- We clean the text and then tokenize it and then lemmatize all the words in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 93364,
     "status": "ok",
     "timestamp": 1743553294206,
     "user": {
      "displayName": "divyendu shekhar",
      "userId": "03175873153716445422"
     },
     "user_tz": 240
    },
    "id": "HkR5m-76FLfm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def clean_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text) if word not in stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "dataset['processed_transcription'] = dataset['transcription'].apply(clean_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset['processed_transcription'], dataset['medical_specialty'], test_size=0.2, random_state=42, stratify=dataset['medical_specialty']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltiKC-3NFLfn"
   },
   "source": [
    "## Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5748,
     "status": "ok",
     "timestamp": 1743553299958,
     "user": {
      "displayName": "divyendu shekhar",
      "userId": "03175873153716445422"
     },
     "user_tz": 240
    },
    "id": "Y6Z7gAenFLfn",
    "outputId": "0e02ff54-a954-4c25-c652-b403a0e50950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7357\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cardiovascular / Pulmonary       0.76      0.91      0.83        74\n",
      "      ENT - Otolaryngology       1.00      0.68      0.81        19\n",
      "          Gastroenterology       0.73      0.84      0.78        45\n",
      "     Hematology - Oncology       0.50      0.11      0.18        18\n",
      "                 Neurology       0.61      0.73      0.67        64\n",
      "   Obstetrics / Gynecology       0.93      0.84      0.88        31\n",
      "             Ophthalmology       0.85      0.65      0.73        17\n",
      "                Orthopedic       0.65      0.82      0.72        71\n",
      "     Pediatrics - Neonatal       0.60      0.21      0.32        14\n",
      "                  Podiatry       0.00      0.00      0.00         9\n",
      "   Psychiatry / Psychology       1.00      0.45      0.62        11\n",
      "                   Urology       0.85      0.83      0.84        47\n",
      "\n",
      "                  accuracy                           0.74       420\n",
      "                 macro avg       0.71      0.59      0.62       420\n",
      "              weighted avg       0.73      0.74      0.71       420\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidataset', TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,4), max_df=0.8, use_idf=True, smooth_idf=True, max_features=2000)),\n",
    "    ('clf', MultinomialNB())  # Train a Naive Bayes classifier\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kNqSNL5FLfn"
   },
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5567,
     "status": "ok",
     "timestamp": 1743553305524,
     "user": {
      "displayName": "divyendu shekhar",
      "userId": "03175873153716445422"
     },
     "user_tz": 240
    },
    "id": "IRiLyriuFLfn",
    "outputId": "c3206494-a8b7-4012-be50-58477f6cc50a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7643\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cardiovascular / Pulmonary       0.77      0.86      0.82        74\n",
      "      ENT - Otolaryngology       0.93      0.68      0.79        19\n",
      "          Gastroenterology       0.71      0.89      0.79        45\n",
      "     Hematology - Oncology       0.60      0.17      0.26        18\n",
      "                 Neurology       0.72      0.75      0.73        64\n",
      "   Obstetrics / Gynecology       0.90      0.87      0.89        31\n",
      "             Ophthalmology       0.87      0.76      0.81        17\n",
      "                Orthopedic       0.67      0.89      0.76        71\n",
      "     Pediatrics - Neonatal       0.71      0.36      0.48        14\n",
      "                  Podiatry       0.00      0.00      0.00         9\n",
      "   Psychiatry / Psychology       1.00      0.55      0.71        11\n",
      "                   Urology       0.93      0.83      0.88        47\n",
      "\n",
      "                  accuracy                           0.76       420\n",
      "                 macro avg       0.73      0.63      0.66       420\n",
      "              weighted avg       0.76      0.76      0.75       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a pipeline with TF-IDF and Logistic Regression\n",
    "lr_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1,4),\n",
    "                             max_df=0.8, use_idf=True, smooth_idf=True, max_features=2000)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, C=1.0, solver='liblinear', multi_class='ovr'))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_y_pred = lr_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "lr_accuracy = accuracy_score(y_test, lr_y_pred)\n",
    "print(f'Logistic Regression Accuracy: {lr_accuracy:.4f}')\n",
    "print(classification_report(y_test, lr_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RT84G_SoFLfn"
   },
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 38084,
     "status": "ok",
     "timestamp": 1743553343609,
     "user": {
      "displayName": "divyendu shekhar",
      "userId": "03175873153716445422"
     },
     "user_tz": 240
    },
    "id": "rZnzIi9vFLfn",
    "outputId": "6035487e-999d-408a-c563-76af29af819a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d_4               │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d_4               │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 300ms/step - accuracy: 0.0916 - loss: 2.4890 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 2/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.0843 - loss: 2.4895 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 3/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.0969 - loss: 2.4873 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 4/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.0733 - loss: 2.4881 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 5/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.0945 - loss: 2.4842 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 6/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.0801 - loss: 2.4943 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 7/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.0758 - loss: 2.4853 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 8/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.0892 - loss: 2.4830 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 9/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.0848 - loss: 2.4932 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 10/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.0784 - loss: 2.4917 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 11/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.0809 - loss: 2.4881 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 12/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0872 - loss: 2.4863 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 13/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.1014 - loss: 2.4827 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 14/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.0830 - loss: 2.4913 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 15/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.0841 - loss: 2.4863 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 16/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.0833 - loss: 2.4864 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 17/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.0872 - loss: 2.4905 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 18/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.0764 - loss: 2.4870 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 19/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.0862 - loss: 2.4915 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "Epoch 20/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.0926 - loss: 2.4882 - val_accuracy: 0.0893 - val_loss: 2.4891\n",
      "CNN Model Accuracy: 0.0714\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cardiovascular / Pulmonary       0.22      0.03      0.05        74\n",
      "      ENT - Otolaryngology       0.00      0.00      0.00        19\n",
      "          Gastroenterology       0.00      0.00      0.00        45\n",
      "     Hematology - Oncology       0.00      0.00      0.00        18\n",
      "                 Neurology       0.00      0.00      0.00        64\n",
      "   Obstetrics / Gynecology       0.07      0.90      0.13        31\n",
      "             Ophthalmology       0.00      0.00      0.00        17\n",
      "                Orthopedic       0.00      0.00      0.00        71\n",
      "     Pediatrics - Neonatal       0.00      0.00      0.00        14\n",
      "                  Podiatry       0.00      0.00      0.00         9\n",
      "   Psychiatry / Psychology       0.00      0.00      0.00        11\n",
      "                   Urology       0.00      0.00      0.00        47\n",
      "\n",
      "                  accuracy                           0.07       420\n",
      "                 macro avg       0.02      0.08      0.01       420\n",
      "              weighted avg       0.04      0.07      0.02       420\n",
      "\n",
      "Naive Bayes Accuracy: 0.7357\n",
      "Logistic Regression Accuracy: 0.7643\n",
      "CNN Accuracy: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Implement CNN model for text classification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "max_features = 50000  # Max vocabulary size\n",
    "maxlen = 1000  # Max sequence length\n",
    "embedding_dims = 500  # Embedding dimension\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "# Convert text to sequences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert to categorical\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train_encoded)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test_encoded)\n",
    "\n",
    "# Build the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    Embedding(max_features, embedding_dims, input_length=maxlen),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_pad,\n",
    "    y_train_cat,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, cnn_accuracy = model.evaluate(X_test_pad, y_test_cat, verbose=0)\n",
    "print(f'CNN Model Accuracy: {cnn_accuracy:.4f}')\n",
    "\n",
    "# Get predictions\n",
    "y_pred_probs = model.predict(X_test_pad)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_labels))\n",
    "\n",
    "# Compare with previous models\n",
    "print(f'Naive Bayes Accuracy: {accuracy:.4f}')\n",
    "print(f'Logistic Regression Accuracy: {lr_accuracy:.4f}')\n",
    "print(f'CNN Accuracy: {cnn_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEcqkuNuFLfn"
   },
   "source": [
    "## Transformers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 229262,
     "status": "ok",
     "timestamp": 1743553572884,
     "user": {
      "displayName": "divyendu shekhar",
      "userId": "03175873153716445422"
     },
     "user_tz": 240
    },
    "id": "QoysXdbyFLfo",
    "outputId": "762ec47c-4a5c-4996-e7c1-aca320bda50b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42/42 [==============================] - 34s 486ms/step - loss: 2.3205 - accuracy: 0.2004 - val_loss: 2.1908 - val_accuracy: 0.3244\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 19s 446ms/step - loss: 1.7935 - accuracy: 0.5283 - val_loss: 1.4461 - val_accuracy: 0.6458\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 19s 443ms/step - loss: 1.1722 - accuracy: 0.7273 - val_loss: 1.0081 - val_accuracy: 0.7530\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 18s 437ms/step - loss: 0.8003 - accuracy: 0.8077 - val_loss: 0.8272 - val_accuracy: 0.7589\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 20s 468ms/step - loss: 0.6014 - accuracy: 0.8495 - val_loss: 0.6813 - val_accuracy: 0.8006\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 20s 472ms/step - loss: 0.4905 - accuracy: 0.8778 - val_loss: 0.6766 - val_accuracy: 0.7857\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 20s 472ms/step - loss: 0.4172 - accuracy: 0.8793 - val_loss: 0.6838 - val_accuracy: 0.7827\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 19s 443ms/step - loss: 0.3651 - accuracy: 0.8845 - val_loss: 0.6909 - val_accuracy: 0.7649\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 18s 440ms/step - loss: 0.3211 - accuracy: 0.8987 - val_loss: 0.7232 - val_accuracy: 0.7827\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 20s 470ms/step - loss: 0.2948 - accuracy: 0.9001 - val_loss: 0.7253 - val_accuracy: 0.7530\n",
      "Transformer Model Accuracy: 0.7905\n",
      "14/14 [==============================] - 4s 119ms/step\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cardiovascular / Pulmonary       0.89      0.88      0.88        74\n",
      "      ENT - Otolaryngology       0.89      0.84      0.86        19\n",
      "          Gastroenterology       0.83      0.84      0.84        45\n",
      "     Hematology - Oncology       0.55      0.33      0.41        18\n",
      "                 Neurology       0.68      0.78      0.72        64\n",
      "   Obstetrics / Gynecology       0.88      0.94      0.91        31\n",
      "             Ophthalmology       0.85      1.00      0.92        17\n",
      "                Orthopedic       0.76      0.62      0.68        71\n",
      "     Pediatrics - Neonatal       0.57      0.57      0.57        14\n",
      "                  Podiatry       0.58      0.78      0.67         9\n",
      "   Psychiatry / Psychology       1.00      0.91      0.95        11\n",
      "                   Urology       0.82      0.89      0.86        47\n",
      "\n",
      "                  accuracy                           0.79       420\n",
      "                 macro avg       0.77      0.78      0.77       420\n",
      "              weighted avg       0.79      0.79      0.79       420\n",
      "\n",
      "Naive Bayes Accuracy: 0.7357\n",
      "Logistic Regression Accuracy: 0.7643\n",
      "CNN Accuracy: 0.0714\n",
      "Transformer Accuracy: 0.7905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from tf_keras.optimizers.legacy import Adam\n",
    "from tf_keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from tf_keras import mixed_precision\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Parameters\n",
    "max_len = 256  # Max sequence length\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Load a pre-trained BERT model and tokenizer from Hugging Face\n",
    "model_name = \"medicalai/ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "transformer_model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_), from_pt=True)\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=max_len, return_tensors=\"tf\")\n",
    "\n",
    "X_train_encoded = encode_texts(X_train)\n",
    "X_test_encoded = encode_texts(X_test)\n",
    "\n",
    "# Fine-tune the model\n",
    "transformer_model.compile(optimizer=Adam(learning_rate=2e-5), loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_transformer = transformer_model.fit(\n",
    "    X_train_encoded['input_ids'], y_train_encoded,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, transformer_accuracy = transformer_model.evaluate(X_test_encoded['input_ids'], y_test_encoded, verbose=0)\n",
    "print(f'Transformer Model Accuracy: {transformer_accuracy:.4f}')\n",
    "\n",
    "y_pred_probs_transformer = transformer_model.predict(X_test_encoded['input_ids'])\n",
    "y_pred_classes_transformer = np.argmax(y_pred_probs_transformer.logits, axis=1)\n",
    "y_pred_labels_transformer = label_encoder.inverse_transform(y_pred_classes_transformer)\n",
    "\n",
    "print(classification_report(y_test, y_pred_labels_transformer))\n",
    "\n",
    "# Compare with previous models\n",
    "print(f'Naive Bayes Accuracy: {accuracy:.4f}')\n",
    "print(f'Logistic Regression Accuracy: {lr_accuracy:.4f}')\n",
    "print(f'CNN Accuracy: {cnn_accuracy:.4f}')\n",
    "print(f'Transformer Accuracy: {transformer_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
