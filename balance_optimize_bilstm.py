# -*- coding: utf-8 -*-
"""balance_optimize_biLSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e8XLJB3lbzmLY-Fm1_1ahwvMMh_lkeUO
"""

# =====================================
# Cell 1: Environment Setup
# =====================================
# 1) Uninstall any existing scikit-learn, scikeras
!pip uninstall -y scikit-learn scikeras

# 2) Install scikit-learn 1.5.2 (which does NOT have the new tag system from 1.6)
#    and also install scikeras (no version pin here, so pip chooses a compatible release).
!pip install scikit-learn==1.5.2 scikeras yake imbalanced-learn

# After this cell finishes, please RESTART the runtime:
# Runtime -> Restart runtime

# =====================================
# Cell 2: Imports & Version Check
# =====================================
import pandas as pd
import numpy as np
import re
import nltk
import yake

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder

from imblearn.over_sampling import RandomOverSampler

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# SciKeras wrapper
from scikeras.wrappers import KerasClassifier

# Download needed NLTK data
nltk.download('stopwords')
nltk.download('wordnet')

# Check versions
import sklearn
import scikeras
print("scikit-learn version:", sklearn.__version__)
print("scikeras version:", scikeras.__version__)

# =====================================
# Cell 3: Load the Dataset
# =====================================
# If using Google Colab + Drive, mount if needed:
from google.colab import drive
drive.mount('/content/drive')

file_path = "/content/drive/MyDrive/mtsamples.csv"  # Adjust path as needed
df = pd.read_csv(file_path)

print("Data shape:", df.shape)
df.head()

# =====================================
# Cell 4: Preprocessing & YAKE
# =====================================

# 1) Drop rows missing critical columns
df.dropna(subset=['transcription', 'medical_specialty', 'sample_name'], inplace=True)

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    # Lowercase
    text = text.lower()
    # Remove non-alphanumeric except spaces
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    # Remove stopwords + lemmatize
    words = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]
    return ' '.join(words)

df['cleaned_transcription'] = df['transcription'].apply(clean_text)
df['cleaned_sample_name']   = df['sample_name'].apply(clean_text)

# Combine for keyword extraction
df['combined_text'] = df['cleaned_transcription'] + " " + df['cleaned_sample_name']

def extract_keywords(text):
    kw_extractor = yake.KeywordExtractor(n=2, top=10)  # up to bi-grams
    keywords = kw_extractor.extract_keywords(text)
    # Return them as space-separated
    return ' '.join([kw[0] for kw in keywords])

df['keywords'] = df['combined_text'].apply(extract_keywords)

# Label encode the 'medical_specialty'
label_encoder = LabelEncoder()
df['encoded_label'] = label_encoder.fit_transform(df['medical_specialty'])

print(f"Number of classes: {len(label_encoder.classes_)}")
df[['keywords', 'medical_specialty', 'encoded_label']].head()

# =====================================
# Cell 5: Train-Test Split
# =====================================
X = df['keywords']
y = df['encoded_label']

# Stratify to keep class distribution
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train size:", len(X_train_raw), "Test size:", len(X_test_raw))

# =====================================
# Cell 6: Random Oversampling
# =====================================
ros = RandomOverSampler(random_state=42)

# Turn X_train_raw into a DataFrame
X_train_df = pd.DataFrame(X_train_raw)
y_train_arr = y_train.values

X_train_resampled, y_train_resampled = ros.fit_resample(X_train_df, y_train_arr)

print("Before oversampling:", X_train_df.shape)
print("After oversampling: ", X_train_resampled.shape)

# Convert the resampled DataFrame back to a list
X_train_list = X_train_resampled.iloc[:, 0].tolist()
y_train_list = y_train_resampled

# =====================================
# Cell 7: Tokenization & Padding
# =====================================
max_words = 20000
max_len   = 50

tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train_list)

X_train_seq = tokenizer.texts_to_sequences(X_train_list)
X_train_seq = pad_sequences(X_train_seq, maxlen=max_len)

X_test_seq = tokenizer.texts_to_sequences(X_test_raw)
X_test_seq = pad_sequences(X_test_seq, maxlen=max_len)

# =====================================
# Cell 8: BiLSTM + Attention Model
# =====================================
from tensorflow.keras.layers import (
    Input, Embedding, LSTM, Bidirectional, Attention,
    GlobalAveragePooling1D, Dense, Dropout
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

def create_bilstm_model(learning_rate=0.0005, lstm_units=128, dropout_rate=0.5):
    input_layer = Input(shape=(max_len,))
    emb = Embedding(input_dim=max_words, output_dim=256, input_length=max_len)(input_layer)

    bilstm = Bidirectional(
        LSTM(lstm_units, return_sequences=True,
             dropout=dropout_rate,
             recurrent_dropout=dropout_rate)
    )(emb)

    # Self-attention
    attention_out = Attention()([bilstm, bilstm])

    # Pool the attention outputs
    pooled = GlobalAveragePooling1D()(attention_out)

    # Dense + Dropout
    dense = Dense(128, activation='relu')(pooled)
    drop  = Dropout(dropout_rate)(dense)

    # Output layer
    output = Dense(len(label_encoder.classes_), activation='softmax')(drop)

    model = Model(inputs=input_layer, outputs=output)
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# =====================================
# Cell 9: GridSearch for Hyperparameters
# =====================================
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import GridSearchCV

keras_clf = KerasClassifier(
    model=create_bilstm_model,
    verbose=0
)

# Notice "model__" prefix for any parameters we pass to create_bilstm_model
param_grid = {
    "model__learning_rate": [1e-3, 5e-4],
    "model__lstm_units":    [64, 128],
    "model__dropout_rate":  [0.3, 0.5],
    "epochs":               [5],       # keep small for demo
    "batch_size":           [64, 128]
}

grid = GridSearchCV(
    estimator=keras_clf,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3  # 3-fold cross-validation
)

grid_result = grid.fit(X_train_seq, y_train_list)

print("Best CV Score:", grid_result.best_score_)
print("Best Params:", grid_result.best_params_)

# =====================================
# Cell 10: Evaluate Best Model
# =====================================
best_model = grid_result.best_estimator_.model_
loss, acc = best_model.evaluate(X_test_seq, y_test, verbose=0)
print(f"Test Accuracy: {acc:.4f}")





