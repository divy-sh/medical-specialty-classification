# -*- coding: utf-8 -*-
"""NLP projectipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LyfFX_L5FGbibLyKQJV5YBatO7vG5fgP
"""

pip install yake

import pandas as pd
import numpy as np
import re
import nltk
import yake
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2

# Download required NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

# Load dataset
file_path = "/content/mtsamples.csv"
df = pd.read_csv(file_path)

# Drop missing values
df.dropna(subset=['transcription', 'medical_specialty', 'sample_name'], inplace=True)

# Initialize stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Text preprocessing function
def clean_text(text):
    text = text.lower()  # Lowercasing
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove special characters
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])  # Remove stopwords & lemmatize
    return text

# Apply text cleaning
df['cleaned_transcription'] = df['transcription'].apply(clean_text)
df['cleaned_sample_name'] = df['sample_name'].apply(clean_text)

# Combine transcription and sample name for keyword extraction
df['combined_text'] = df['cleaned_transcription'] + " " + df['cleaned_sample_name']

# YAKE Keyword Extraction
def extract_keywords(text):
    kw_extractor = yake.KeywordExtractor(n=2, top=10)  # Extract top 10 bi-gram keywords
    keywords = kw_extractor.extract_keywords(text)
    return ' '.join([kw[0] for kw in keywords])  # Return keywords as a space-separated string

# Apply YAKE for keyword extraction
df['keywords'] = df['combined_text'].apply(extract_keywords)

# Encode labels
label_encoder = LabelEncoder()
df['encoded_label'] = label_encoder.fit_transform(df['medical_specialty'])

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(
    df['keywords'], df['encoded_label'], test_size=0.2, random_state=42, stratify=df['encoded_label']
)

# Tokenization
max_words = 20000  # Increased vocabulary size
max_len = 50  # Reduced sequence length since keywords are shorter
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)
X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)
X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)

# Build LSTM model
model = Sequential([
    Embedding(input_dim=max_words, output_dim=256, input_length=max_len),
    LSTM(128, return_sequences=True, dropout=0.5, recurrent_dropout=0.5),
    LSTM(64, dropout=0.5, recurrent_dropout=0.5),
    BatchNormalization(),
    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.5),
    Dense(len(label_encoder.classes_), activation='softmax')
])

# Compile model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train model
model.fit(X_train_seq, y_train, epochs=10, batch_size=128, validation_data=(X_test_seq, y_test))

# Evaluate model
loss, accuracy = model.evaluate(X_test_seq, y_test)
print(f'Accuracy: {accuracy:.4f}')



pip install datasets

