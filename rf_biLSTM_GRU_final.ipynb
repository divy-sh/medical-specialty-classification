{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEPCEe6YVmob",
        "outputId": "f3d6a141-6a79-4e80-f322-46f8658c5f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tf-keras) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# install required dependencies\n",
        "%pip install kagglehub\n",
        "%pip install pandas\n",
        "%pip install nltk\n",
        "%pip install sklearn\n",
        "%pip install tensorflow\n",
        "%pip install matplotlib\n",
        "%pip install tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import all the required dependencies\n",
        "import kagglehub\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T3aTXyOVnRW",
        "outputId": "76734330-b7a8-4ae2-9561-ff4c92759efe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"tboyle10/medicaltranscriptions\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "dataset = pd.read_csv(path + \"/mtsamples.csv\")\n",
        "print(\"Head: \", dataset.head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io6OwHsqVnT6",
        "outputId": "e8c76cf6-bde6-4d78-8e43-a78bd3fe95e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/tboyle10/medicaltranscriptions?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.85M/4.85M [00:00<00:00, 32.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/tboyle10/medicaltranscriptions/versions/1\n",
            "Head:  <bound method NDFrame.head of       Unnamed: 0                                        description  \\\n",
            "0              0   A 23-year-old white female presents with comp...   \n",
            "1              1           Consult for laparoscopic gastric bypass.   \n",
            "2              2           Consult for laparoscopic gastric bypass.   \n",
            "3              3                             2-D M-Mode. Doppler.     \n",
            "4              4                                 2-D Echocardiogram   \n",
            "...          ...                                                ...   \n",
            "4994        4994   Patient having severe sinusitis about two to ...   \n",
            "4995        4995   This is a 14-month-old baby boy Caucasian who...   \n",
            "4996        4996   A female for a complete physical and follow u...   \n",
            "4997        4997   Mother states he has been wheezing and coughing.   \n",
            "4998        4998   Acute allergic reaction, etiology uncertain, ...   \n",
            "\n",
            "                medical_specialty                                sample_name  \\\n",
            "0            Allergy / Immunology                         Allergic Rhinitis    \n",
            "1                      Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
            "2                      Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
            "3      Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
            "4      Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
            "...                           ...                                        ...   \n",
            "4994         Allergy / Immunology                         Chronic Sinusitis    \n",
            "4995         Allergy / Immunology      Kawasaki Disease - Discharge Summary    \n",
            "4996         Allergy / Immunology                        Followup on Asthma    \n",
            "4997         Allergy / Immunology                    Asthma in a 5-year-old    \n",
            "4998         Allergy / Immunology                Allergy Evaluation Consult    \n",
            "\n",
            "                                          transcription  \\\n",
            "0     SUBJECTIVE:,  This 23-year-old white female pr...   \n",
            "1     PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
            "2     HISTORY OF PRESENT ILLNESS: , I have seen ABC ...   \n",
            "3     2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
            "4     1.  The left ventricular cavity size and wall ...   \n",
            "...                                                 ...   \n",
            "4994  HISTORY:,  I had the pleasure of meeting and e...   \n",
            "4995  ADMITTING DIAGNOSIS: , Kawasaki disease.,DISCH...   \n",
            "4996  SUBJECTIVE: , This is a 42-year-old white fema...   \n",
            "4997  CHIEF COMPLAINT: , This 5-year-old male presen...   \n",
            "4998  HISTORY: , A 34-year-old male presents today s...   \n",
            "\n",
            "                                               keywords  \n",
            "0     allergy / immunology, allergic rhinitis, aller...  \n",
            "1     bariatrics, laparoscopic gastric bypass, weigh...  \n",
            "2     bariatrics, laparoscopic gastric bypass, heart...  \n",
            "3     cardiovascular / pulmonary, 2-d m-mode, dopple...  \n",
            "4     cardiovascular / pulmonary, 2-d, doppler, echo...  \n",
            "...                                                 ...  \n",
            "4994                                                NaN  \n",
            "4995  allergy / immunology, mucous membranes, conjun...  \n",
            "4996                                                NaN  \n",
            "4997                                                NaN  \n",
            "4998                                                NaN  \n",
            "\n",
            "[4999 rows x 6 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values in specified columns\n",
        "dataset.dropna(subset=['transcription', 'medical_specialty'], inplace=True)\n",
        "\n",
        "# Keep only relevant columns\n",
        "dataset = dataset[['transcription', 'medical_specialty']]\n",
        "\n",
        "# # Filter medical specialties with at least 30 occurrences\n",
        "specialty_counts = dataset['medical_specialty'].value_counts()\n",
        "# valid_specialties = specialty_counts[specialty_counts >= 50].index\n",
        "# dataset = dataset[dataset['medical_specialty'].isin(valid_specialties)]\n",
        "\n",
        "# Strip spaces in 'medical_specialty' column\n",
        "dataset['medical_specialty'] = dataset['medical_specialty'].str.strip()\n",
        "\n",
        "# Remove specific categories\n",
        "excluded_specialties = [\n",
        "    'Surgery',\n",
        "    'SOAP / Chart / Progress Notes',\n",
        "    'Office Notes',\n",
        "    'Consult - History and Phy.',\n",
        "    'Emergency Room Reports',\n",
        "    'Discharge Summary',\n",
        "    'Pain Management',\n",
        "    'General Medicine',\n",
        "    'Radiology',\n",
        "]\n",
        "\n",
        "dataset = dataset[~dataset['medical_specialty'].isin(excluded_specialties)]\n",
        "\n",
        "# Define category mapping to merge similar categories\n",
        "category_mapping = {\n",
        "    'Neurosurgery': 'Neurology',\n",
        "    'Nephrology': 'Urology',\n",
        "}\n",
        "\n",
        "# Apply category mapping\n",
        "dataset['medical_specialty'] = dataset['medical_specialty'].replace(category_mapping)\n",
        "\n",
        "# Display counts for each category\n",
        "for i, (category_name, category) in enumerate(dataset.groupby(\"medical_specialty\")):\n",
        "    print(f\"Category {i}: {category_name}: {len(category)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sELVTs-nVnW0",
        "outputId": "020155e4-95a5-4026-8b29-a9e0b4f9d970"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category 0: Allergy / Immunology: 7\n",
            "Category 1: Autopsy: 8\n",
            "Category 2: Bariatrics: 18\n",
            "Category 3: Cardiovascular / Pulmonary: 371\n",
            "Category 4: Chiropractic: 14\n",
            "Category 5: Cosmetic / Plastic Surgery: 27\n",
            "Category 6: Dentistry: 27\n",
            "Category 7: Dermatology: 29\n",
            "Category 8: Diets and Nutritions: 10\n",
            "Category 9: ENT - Otolaryngology: 96\n",
            "Category 10: Endocrinology: 19\n",
            "Category 11: Gastroenterology: 224\n",
            "Category 12: Hematology - Oncology: 90\n",
            "Category 13: Hospice - Palliative Care: 6\n",
            "Category 14: IME-QME-Work Comp etc.: 16\n",
            "Category 15: Lab Medicine - Pathology: 8\n",
            "Category 16: Letters: 23\n",
            "Category 17: Neurology: 317\n",
            "Category 18: Obstetrics / Gynecology: 155\n",
            "Category 19: Ophthalmology: 83\n",
            "Category 20: Orthopedic: 355\n",
            "Category 21: Pediatrics - Neonatal: 70\n",
            "Category 22: Physical Medicine - Rehab: 21\n",
            "Category 23: Podiatry: 47\n",
            "Category 24: Psychiatry / Psychology: 53\n",
            "Category 25: Rheumatology: 10\n",
            "Category 26: Sleep Medicine: 20\n",
            "Category 27: Speech - Language: 9\n",
            "Category 28: Urology: 237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper: Get synonyms from WordNet\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonym = lemma.name().replace('_', ' ')\n",
        "            if synonym.lower() != word.lower():\n",
        "                synonyms.add(synonym)\n",
        "    return list(synonyms)\n",
        "\n",
        "# Synonym Replacement with random n\n",
        "def synonym_replacement(text, n=None):\n",
        "    words = word_tokenize(text)\n",
        "    new_words = words.copy()\n",
        "    eligible_words = list(set([word for word in words if word.isalpha()]))\n",
        "    random.shuffle(eligible_words)\n",
        "\n",
        "    if n is None:\n",
        "        n = random.randint(1, min(3, len(eligible_words)))\n",
        "\n",
        "    num_replaced = 0\n",
        "    for word in eligible_words:\n",
        "        synonyms = get_synonyms(word)\n",
        "        if synonyms:\n",
        "            synonym = random.choice(synonyms)\n",
        "            new_words = [synonym if w == word else w for w in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# Word Dropout\n",
        "def word_dropout(text, dropout_prob=0.1):\n",
        "    words = word_tokenize(text)\n",
        "    new_words = [word for word in words if random.random() > dropout_prob]\n",
        "    return ' '.join(new_words) if new_words else text\n",
        "\n",
        "# Random Swap\n",
        "def random_swap(text, n=1):\n",
        "    words = word_tokenize(text)\n",
        "    if len(words) < 2:\n",
        "        return text\n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply 1–2 random augmentations\n",
        "def augment_text_randomly(text):\n",
        "    aug_functions = [\n",
        "        synonym_replacement,\n",
        "        word_dropout,\n",
        "        random_swap\n",
        "    ]\n",
        "    num_augs = random.randint(1, 2)\n",
        "    selected_augs = random.sample(aug_functions, num_augs)\n",
        "    for aug in selected_augs:\n",
        "        text = aug(text)\n",
        "    return text\n",
        "\n",
        "# Parallel augmentation per class\n",
        "def augment_class(label_samples_tuple):\n",
        "    label, samples_needed, class_samples = label_samples_tuple\n",
        "    samples = class_samples.sample(n=samples_needed, replace=True)\n",
        "    texts = samples['transcription'].tolist()\n",
        "\n",
        "    with Pool(cpu_count()) as p:\n",
        "        augmented_texts = p.map(augment_text_randomly, texts)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'transcription': augmented_texts,\n",
        "        'medical_specialty': label\n",
        "    })\n",
        "\n",
        "specialty_counts = dataset['medical_specialty'].value_counts()\n",
        "max_count = specialty_counts.max()\n",
        "\n",
        "# Prepare augmentation tasks\n",
        "tasks = []\n",
        "for label, count in specialty_counts.items():\n",
        "    if count < max_count:\n",
        "        samples_needed = max_count - count\n",
        "        class_samples = dataset[dataset['medical_specialty'] == label]\n",
        "        tasks.append((label, samples_needed, class_samples))\n",
        "\n",
        "# Run augmentations\n",
        "augmented_dfs = [augment_class(task) for task in tasks]\n",
        "\n",
        "# Combine datasets\n",
        "augmented_df = pd.concat(augmented_dfs, ignore_index=True)\n",
        "dataset = pd.concat([dataset, augmented_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "6Chi2pHNUaA3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "lemmatizer = None\n",
        "stop_words = None\n",
        "\n",
        "def init_worker():\n",
        "    global lemmatizer, stop_words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.strip().lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def apply_multiprocessing(series, func, workers=None):\n",
        "    with Pool(processes=workers or cpu_count(), initializer=init_worker) as pool:\n",
        "        results = pool.map(func, series)\n",
        "    return results\n",
        "\n",
        "# Use multiprocessing to speed it up\n",
        "dataset['processed_transcription'] = apply_multiprocessing(dataset['transcription'], clean_text)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    dataset['processed_transcription'], dataset['medical_specialty'], test_size=0.2, random_state=42, stratify=dataset['medical_specialty']\n",
        ")"
      ],
      "metadata": {
        "id": "Bn309rzeVnYx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "6YJBWfPVbm8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "rf_pipeline = Pipeline([('tfidf', TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 5), max_df=0.7, max_features=2000)),\n",
        "                        ('clf', RandomForestClassifier(n_estimators=150, random_state=30))])\n",
        "\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "rf_y_pred = rf_pipeline.predict(X_test)\n",
        "\n",
        "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
        "print(f'Random Forest Accuracy: {rf_accuracy:.4f}')\n",
        "print(classification_report(y_test, rf_y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkZiDCHDVndw",
        "outputId": "fc047cc9-1001-45c0-a671-c1b0b1770087"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.8954\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "      Allergy / Immunology       0.99      1.00      0.99        74\n",
            "                   Autopsy       1.00      0.99      0.99        74\n",
            "                Bariatrics       0.95      0.97      0.96        74\n",
            "Cardiovascular / Pulmonary       0.82      0.85      0.83        74\n",
            "              Chiropractic       0.78      0.91      0.84        74\n",
            "Cosmetic / Plastic Surgery       0.86      0.88      0.87        74\n",
            "                 Dentistry       1.00      1.00      1.00        75\n",
            "               Dermatology       0.90      0.92      0.91        75\n",
            "      Diets and Nutritions       1.00      1.00      1.00        74\n",
            "      ENT - Otolaryngology       0.86      0.91      0.88        74\n",
            "             Endocrinology       0.93      0.99      0.95        75\n",
            "          Gastroenterology       0.87      0.73      0.79        74\n",
            "     Hematology - Oncology       0.81      0.78      0.79        74\n",
            " Hospice - Palliative Care       0.99      1.00      0.99        74\n",
            "    IME-QME-Work Comp etc.       0.87      0.81      0.84        75\n",
            "  Lab Medicine - Pathology       0.99      0.99      0.99        74\n",
            "                   Letters       0.84      0.84      0.84        74\n",
            "                 Neurology       0.62      0.55      0.59        74\n",
            "   Obstetrics / Gynecology       0.93      0.89      0.91        74\n",
            "             Ophthalmology       0.87      0.88      0.87        75\n",
            "                Orthopedic       0.64      0.56      0.60        75\n",
            "     Pediatrics - Neonatal       0.89      0.77      0.83        74\n",
            " Physical Medicine - Rehab       0.95      1.00      0.97        74\n",
            "                  Podiatry       0.89      0.96      0.92        74\n",
            "   Psychiatry / Psychology       0.96      0.97      0.97        74\n",
            "              Rheumatology       0.97      1.00      0.99        74\n",
            "            Sleep Medicine       0.95      0.99      0.97        74\n",
            "         Speech - Language       0.97      1.00      0.99        74\n",
            "                   Urology       0.85      0.84      0.84        74\n",
            "\n",
            "                  accuracy                           0.90      2152\n",
            "                 macro avg       0.89      0.90      0.89      2152\n",
            "              weighted avg       0.89      0.90      0.89      2152\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bi-LSTM"
      ],
      "metadata": {
        "id": "pAx0gMWPWxa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "max_words = 50000\n",
        "max_len = 2000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_enc = label_encoder.fit_transform(y_train)\n",
        "y_test_enc = label_encoder.transform(y_test)\n",
        "\n",
        "y_train_cat = tf.keras.utils.to_categorical(y_train_enc)\n",
        "y_test_cat = tf.keras.utils.to_categorical(y_test_enc)\n"
      ],
      "metadata": {
        "id": "FbBFMNOKVnfp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, GlobalMaxPooling1D, LayerNormalization\n",
        "\n",
        "model = Sequential([Embedding(input_dim=max_words, output_dim=128, input_length=max_len), Bidirectional(LSTM(64, return_sequences=True, dropout=0.3)),\n",
        "                    LayerNormalization(), GlobalMaxPooling1D(), Dropout(0.4),\n",
        "                    Dense(64, activation='relu'), Dropout(0.4), Dense(y_train_cat.shape[1], activation='softmax')])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-CoEDV0Vnhv",
        "outputId": "b78826c1-7947-45e5-e0d7-1c36ac1c8c40"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_pad, y_train_cat, epochs=20, batch_size=32, validation_split=0.2, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nthC1vdVnmT",
        "outputId": "40bcdb33-e9e8-4e8a-dd3a-63ff5be0f929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m208/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m30s\u001b[0m 4s/step - accuracy: 0.0551 - loss: 3.4543"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test_pad, y_test_cat, verbose=0)\n",
        "print(\"BiLSTM Accuracy (test): \", accuracy)\n",
        "\n",
        "y_pred_probs = model.predict(X_test_pad)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "y_true_classes = np.argmax(y_test_cat, axis=1)\n",
        "\n",
        "# get original string names\n",
        "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
        "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_labels, y_pred_labels))"
      ],
      "metadata": {
        "id": "0SXQJhJKVnrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "ZbAoqGxEWXIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Bidirectional, Dense, Dropout\n",
        "from tensorflow.keras.layers import LayerNormalization, GlobalMaxPooling1D\n",
        "\n",
        "model = Sequential([Embedding(max_words, 128, input_length=max_len), Bidirectional(GRU(64, return_sequences=True)),\n",
        "    LayerNormalization(), GlobalMaxPooling1D(), Dropout(0.4),\n",
        "    Dense(64, activation='relu'), Dropout(0.4), Dense(y_train_cat.shape[1], activation='softmax')])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SVqJ0cr3VnvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_pad, y_train_cat, epochs=20, batch_size=32, validation_split=0.2, verbose=1)"
      ],
      "metadata": {
        "id": "QMAonno4Wd72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test_pad, y_test_cat, verbose=0)\n",
        "print(\"GRU Accuracy (test): \", accuracy)\n",
        "\n",
        "y_pred_probs = model.predict(X_test_pad)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "y_true_classes = np.argmax(y_test_cat, axis=1)\n",
        "\n",
        "# Decode to string labels\n",
        "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
        "y_true_labels = label_encoder.inverse_transform(y_true_classes)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_labels, y_pred_labels))"
      ],
      "metadata": {
        "id": "ebhtCNURWeAf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}