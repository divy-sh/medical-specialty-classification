# -*- coding: utf-8 -*-
"""BERTembed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s-2tO9Cd1D7ibaUxvfEoZeJ0NJ8mc1tZ
"""

!pip install sentence-transformers spacy scispacy

!pip install -U numpy
!pip install -U spacy scispacy

!pip install --upgrade numpy
!pip install --upgrade spacy
!pip install sentence-transformers scispacy

!pip install --upgrade numpy spacy scispacy

!pip uninstall numpy cupy -y
!pip install numpy==1.26.0 cupy-cuda12x==12.2.0

!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz



import spacy
import torch
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sentence_transformers import SentenceTransformer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2

# Load SciSpacy model for NER
nlp = spacy.load("en_core_sci_sm")

# Load dataset
file_path = "/content/mtsamples.csv"
df = pd.read_csv(file_path)

df.dropna(subset=['transcription', 'medical_specialty', 'sample_name'], inplace=True)

# Initialize stopwords and lemmatizer
nltk.download('stopwords')
nltk.download('wordnet')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Text preprocessing function
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])
    return text

# Apply text cleaning
df['cleaned_transcription'] = df['transcription'].apply(clean_text)
df['cleaned_sample_name'] = df['sample_name'].apply(clean_text)

df['combined_text'] = df['cleaned_transcription'] + " " + df['cleaned_sample_name']

# Extract medical terms using Named Entity Recognition (NER)
def extract_medical_terms(text):
    doc = nlp(text)
    return ' '.join([ent.text for ent in doc.ents if ent.label_ in ["DISEASE", "TREATMENT", "SYMPTOM"]])

df['medical_terms'] = df['cleaned_transcription'].apply(extract_medical_terms)

# Load BERT model
bert_model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate BERT embeddings
def get_bert_embedding(text):
    return bert_model.encode(text).tolist()

df['bert_embeddings'] = df['combined_text'].apply(get_bert_embedding)

# Encode labels
label_encoder = LabelEncoder()
df['encoded_label'] = label_encoder.fit_transform(df['medical_specialty'])

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(
    df['bert_embeddings'].tolist(), df['encoded_label'], test_size=0.2, random_state=42, stratify=df['encoded_label']
)

# Convert to NumPy arrays
X_train = np.array(X_train)
X_test = np.array(X_test)

# Build MLP Model
model = Sequential([
    Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.001)),
    Dropout(0.5),
    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.5),
    BatchNormalization(),
    Dense(len(label_encoder.classes_), activation='softmax')
])

# Compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, epochs=15, batch_size=128, validation_data=(X_test, y_test))

# Evaluate model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Accuracy: {accuracy:.4f}')