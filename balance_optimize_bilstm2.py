# -*- coding: utf-8 -*-
"""balance_optimize_biLSTM2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UhBlm0lHDX0eKzBv4MkcfgHyFSZOi5dn
"""

# =====================================
# Cell 1: Environment Setup
# =====================================
# 1) Remove existing scikit-learn/scikeras that might be >=1.6
!pip uninstall -y scikit-learn scikeras

# 2) Install scikit-learn==1.5.2 (compatible with scikeras avoiding __sklearn_tags__ error)
#    plus scikeras, yake, imbalanced-learn.
!pip install scikit-learn==1.5.2 scikeras yake imbalanced-learn

# After installation, go to "Runtime" -> "Restart runtime" in Colab.

# =====================================
# Cell 2: Imports & Version Check
# =====================================
import pandas as pd
import numpy as np
import re
import nltk
import yake

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder

# For pipelines & oversampling
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import RandomOverSampler

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from scikeras.wrappers import KerasClassifier

# Download needed NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

# Check versions
import sklearn, scikeras
print("scikit-learn version:", sklearn.__version__)
print("scikeras version:", scikeras.__version__)

# =====================================
# Cell 3: Load Dataset
# =====================================
from google.colab import drive
drive.mount('/content/drive')

file_path = "/content/drive/MyDrive/mtsamples.csv"  # adjust if needed
df = pd.read_csv(file_path)

print("Data shape:", df.shape)
df.head()

# =====================================
# Cell 4: Preprocessing & YAKE
# =====================================
df.dropna(subset=['transcription', 'medical_specialty', 'sample_name'], inplace=True)

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    return ' '.join(
        lemmatizer.lemmatize(w)
        for w in text.split()
        if w not in stop_words
    )

df['cleaned_transcription'] = df['transcription'].apply(clean_text)
df['cleaned_sample_name']   = df['sample_name'].apply(clean_text)
df['combined_text']         = df['cleaned_transcription'] + " " + df['cleaned_sample_name']

def extract_keywords(text):
    kw_extractor = yake.KeywordExtractor(n=2, top=10)
    keywords = kw_extractor.extract_keywords(text)
    return ' '.join(kw[0] for kw in keywords)

df['keywords'] = df['combined_text'].apply(extract_keywords)
df.head()

# =====================================
# Cell 5: Encode Labels & Train/Test Split
# =====================================
label_encoder = LabelEncoder()
df['encoded_label'] = label_encoder.fit_transform(df['medical_specialty'])

X = df['keywords']
y = df['encoded_label']

X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train size:", len(X_train_raw), "Test size:", len(X_test_raw))
print("Number of classes:", len(label_encoder.classes_))

# =====================================
# Cell 6: Tokenization & Padding
# =====================================
max_words = 20000
max_len   = 50

tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train_raw)

X_train_seq = tokenizer.texts_to_sequences(X_train_raw)
X_train_seq = pad_sequences(X_train_seq, maxlen=max_len)

X_test_seq = tokenizer.texts_to_sequences(X_test_raw)
X_test_seq = pad_sequences(X_test_seq, maxlen=max_len)

# =====================================
# Cell 7: BiLSTM + Attention Model
# =====================================
from tensorflow.keras.layers import (
    Input, Embedding, LSTM, Bidirectional,
    Attention, GlobalAveragePooling1D, Dense, Dropout
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

def create_bilstm_model(learning_rate=1e-3, lstm_units=64, dropout_rate=0.3):
    input_layer = Input(shape=(max_len,))
    emb = Embedding(input_dim=max_words, output_dim=256, input_length=max_len)(input_layer)

    bilstm = Bidirectional(
        LSTM(lstm_units, return_sequences=True,
             dropout=dropout_rate, recurrent_dropout=dropout_rate)
    )(emb)

    attn_out = Attention()([bilstm, bilstm])
    pooled = GlobalAveragePooling1D()(attn_out)

    dense = Dense(128, activation='relu')(pooled)
    drop  = Dropout(dropout_rate)(dense)

    output = Dense(len(label_encoder.classes_), activation='softmax')(drop)

    model = Model(inputs=input_layer, outputs=output)
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# =====================================
# Cell 8: Pipeline with RandomOverSampler + KerasClassifier
# =====================================
from imblearn.pipeline import Pipeline

keras_clf = KerasClassifier(
    model=create_bilstm_model,
    verbose=0  # reduce logs for speed
)

pipeline = Pipeline([
    ("oversample", RandomOverSampler(random_state=42)),
    ("clf", keras_clf)
])

# =====================================
# Cell 9: RandomizedSearchCV (Faster)
# =====================================
from sklearn.model_selection import RandomizedSearchCV

# We'll define modest hyperparameter distributions:
param_distributions = {
    "clf__model__learning_rate": [1e-3, 5e-4, 1e-4],    # 3 possibilities
    "clf__model__lstm_units":    [64, 128],             # 2 possibilities
    "clf__model__dropout_rate":  [0.3, 0.5],            # 2 possibilities
    "clf__epochs":               [3, 5],                # 2 possibilities
    "clf__batch_size":           [64, 128]              # 2 possibilities
}

# That is 3 * 2 * 2 * 2 * 2 = 48 total combos if we did a full grid.
# We'll pick n_iter=6 for demonstration, so we only sample 6 combos at random.

random_search = RandomizedSearchCV(
    estimator=pipeline,
    param_distributions=param_distributions,
    n_iter=6,             # Only 6 random combos
    scoring='accuracy',
    cv=2,                 # Just 2-fold CV for speed
    verbose=1,            # Show progress
    random_state=42       # Reproducibility
)

random_result = random_search.fit(X_train_seq, y_train)
print("Best CV Score:", random_result.best_score_)
print("Best Params:", random_result.best_params_)

# =====================================
# Cell 10: Test Evaluation
# =====================================
best_pipeline = random_result.best_estimator_

test_acc = best_pipeline.score(X_test_seq, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

# If you want to see which random combo was chosen:
print("Chosen Hyperparams:", random_result.best_params_)

















































