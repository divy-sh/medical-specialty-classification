# -*- coding: utf-8 -*-
"""class_weighting_biLSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/175C4Mc0RzzrIvptWLfg_AChtWw0g1r6j
"""

# =================================================
# Cell 1: Environment Setup
# =================================================
# 1) Uninstall existing scikit-learn/scikeras
!pip uninstall -y scikit-learn scikeras

# 2) Install specific versions:
#    - scikit-learn==1.5.2 to avoid '__sklearn_tags__' error with SciKeras
#    - scikeras, yake, imbalanced-learn (though we won't use oversampling here, it's harmless)
!pip install scikit-learn==1.5.2 scikeras yake imbalanced-learn

# After installing, RESTART your runtime (Runtime -> Restart runtime) for changes to take effect.

# =================================================
# Cell 2: Imports & Version Check
# =================================================
import pandas as pd
import numpy as np
import re
import nltk
import yake

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# SciKeras
from scikeras.wrappers import KerasClassifier

nltk.download('stopwords')
nltk.download('wordnet')

# Check versions
import sklearn
import scikeras
print("scikit-learn version:", sklearn.__version__)
print("scikeras version:", scikeras.__version__)

# =================================================
# Cell 3: Load the Dataset
# =================================================
# If using Google Colab + Drive:
from google.colab import drive
drive.mount('/content/drive')

file_path = "/content/drive/MyDrive/mtsamples.csv"  # Adjust path if needed
df = pd.read_csv(file_path)
print("Data shape:", df.shape)
df.head()

# =================================================
# Cell 4: Preprocessing & YAKE
# =================================================
df.dropna(subset=['transcription', 'medical_specialty', 'sample_name'], inplace=True)

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    return ' '.join(
        lemmatizer.lemmatize(word)
        for word in text.split()
        if word not in stop_words
    )

df['cleaned_transcription'] = df['transcription'].apply(clean_text)
df['cleaned_sample_name']   = df['sample_name'].apply(clean_text)
df['combined_text']         = df['cleaned_transcription'] + " " + df['cleaned_sample_name']

def extract_keywords(text):
    kw_extractor = yake.KeywordExtractor(n=2, top=10)
    keywords = kw_extractor.extract_keywords(text)
    return ' '.join(kw[0] for kw in keywords)

# For demonstration, let's keep using YAKE:
df['keywords'] = df['combined_text'].apply(extract_keywords)
df.head()

# =================================================
# Cell 5: Label Encode & Train-Test Split
# =================================================
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
df['encoded_label'] = label_encoder.fit_transform(df['medical_specialty'])

X = df['keywords']
y = df['encoded_label']

X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train size:", len(X_train_raw))
print("Test size: ", len(X_test_raw))
print("Number of classes:", len(label_encoder.classes_))

# =================================================
# Cell 6: Tokenization & Padding
# =================================================
max_words = 20000
max_len   = 50

tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train_raw)

X_train_seq = tokenizer.texts_to_sequences(X_train_raw)
X_train_seq = pad_sequences(X_train_seq, maxlen=max_len)

X_test_seq = tokenizer.texts_to_sequences(X_test_raw)
X_test_seq = pad_sequences(X_test_seq, maxlen=max_len)

print("X_train_seq shape:", X_train_seq.shape)
print("X_test_seq shape: ", X_test_seq.shape)

# =================================================
# Cell 7: BiLSTM + Attention Model
# =================================================
from tensorflow.keras.layers import (
    Input, Embedding, LSTM, Bidirectional,
    Attention, GlobalAveragePooling1D, Dense, Dropout
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

def create_bilstm_model(learning_rate=1e-3, lstm_units=64, dropout_rate=0.3):
    input_layer = Input(shape=(max_len,))
    emb = Embedding(input_dim=max_words, output_dim=256, input_length=max_len)(input_layer)

    bilstm = Bidirectional(
        LSTM(lstm_units, return_sequences=True,
             dropout=dropout_rate, recurrent_dropout=dropout_rate)
    )(emb)

    attn_out = Attention()([bilstm, bilstm])
    pooled   = GlobalAveragePooling1D()(attn_out)

    dense = Dense(128, activation='relu')(pooled)
    drop  = Dropout(dropout_rate)(dense)

    output = Dense(len(label_encoder.classes_), activation='softmax')(drop)

    model = Model(inputs=input_layer, outputs=output)
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# =================================================
# Cell 8: KerasClassifier with class_weight
# =================================================
keras_clf = KerasClassifier(
    model=create_bilstm_model,
    verbose=0,
    class_weight="balanced"  # <--- Important change
)

# =================================================
# Cell 9: RandomizedSearchCV with class weighting
# =================================================
from sklearn.model_selection import RandomizedSearchCV

# We'll explore a few hyperparameters
param_distributions = {
    "model__learning_rate": [1e-3, 5e-4, 1e-4],
    "model__lstm_units":    [64, 128],
    "model__dropout_rate":  [0.3, 0.5],
    "epochs":               [3, 5],
    "batch_size":           [64, 128]
}

# That is 3 * 2 * 2 * 2 * 2 = 48 possible combos if we did a full grid,
# but we'll randomly pick, say, 6 combos for demonstration:
random_search = RandomizedSearchCV(
    estimator=keras_clf,
    param_distributions=param_distributions,
    n_iter=6,          # sample 6 random combos
    scoring='accuracy',
    cv=2,              # 2-fold CV for quicker runs
    verbose=1,
    random_state=42
)

# Fit on unmodified training data
random_result = random_search.fit(X_train_seq, y_train)

print("Best CV Score:", random_result.best_score_)
print("Best Params:", random_result.best_params_)

# =================================================
# Cell 10: Evaluate on Test Set
# =================================================
best_clf = random_result.best_estimator_

test_acc = best_clf.score(X_test_seq, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

print("Chosen Hyperparams:", random_result.best_params_)









